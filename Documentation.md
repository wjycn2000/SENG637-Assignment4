




## 2 Mutation Testing (50)

### 2.1 Introduction

Mutation testing is a fault-based software testing technique used to evaluate the quality and effectiveness of test cases. It involves introducing small changes (mutations) to the source code and running the test suite to determine whether these mutations are detected (i.e., "killed") by the tests. If a mutation is not detected, it is said to have "survived," indicating a potential weakness in the test suite.

For this assignment, mutation testing was conducted on two core classes from the JFreeChart library: `Range` and `DataUtilities`. The objective was to evaluate the strength of the original test suites and then iteratively improve them by designing additional test cases that target surviving or undetected mutants.

We used PIT (Pitest), a mutation testing tool, integrated with Eclipse through PITclipse. The mutation results were analyzed in detail, and targeted test cases were created to kill previously surviving mutants. Special attention was paid to edge cases, boundary conditions, logical branches, and equivalent mutants.

This section presents a comprehensive analysis of the mutation testing efforts, strategies used to improve coverage, challenges encountered, and lessons learned throughout the process.

### 2.2 Analysis of at least 10 mutants produced by Pitest for the Range class, and how they are killed or not by your original test suite

Below is a detailed analysis of ten specific mutants generated by PIT for the `Range` class, including whether they were killed or survived and how they were addressed:

1. **Mutant in `getCentralValue()` (changed `+` to `-`)**  
   - **Mutation**: `(lower + upper) / 2` mutated to `(lower - upper) / 2`  
   - **Status**: ✅ Killed  
   - **Reason**: Original test `testGetCentralValue()` asserts exact mid-point; any deviation fails.

2. **Mutant in `constrain()` (changed `value > upper` to `value >= upper`)**  
   - **Status**: ✅ Killed  
   - **Reason**: The test suite includes `value == upper`, which correctly exercises the edge condition.

3. **Mutant in `constrain()` (changed `value < lower` to `value <= lower`)**  
   - **Status**: ✅ Killed  
   - **Reason**: Tests check `value == lower` and assert no constraint happens.

4. **Mutant in `equals()` (reversed logical AND condition)**  
   - **Status**: ✅ Killed  
   - **Reason**: Tests explicitly check inequality when only one bound differs.

5. **Mutant in `intersects()` (changed condition to `b1 < lower` instead of `b1 <= lower`)**  
   - **Status**: ❌ Survived initially  
   - **Reason**: Original tests didn’t cover boundary where `b1 == lower`.  
   - **Fix**: Added `testIntersects_ExactLowerBound_Fixed()` to assert non-intersection.

6. **Mutant in `shift()` (removed check for `allowZeroCrossing == false`)**  
   - **Status**: ❌ Survived  
   - **Reason**: No test originally verified shift with negative delta and crossing disabled.  
   - **Fix**: Added `testShift_NegativeDelta_ZeroCrossingFalse()` to verify clamping behavior.

7. **Mutant in constructor (removed exception for `lower > upper`)**  
   - **Status**: ✅ Killed  
   - **Reason**: Original test already expected `IllegalArgumentException` on invalid input.

8. **Mutant in `scale()` (changed `length * factor` to `length / factor`)**  
   - **Status**: ✅ Killed  
   - **Reason**: Central value and bounds tested in various scale scenarios including decimals.

9. **Mutant in `isNaNRange()` (replaced `Double.isNaN(lower)` with `false`)**  
   - **Status**: ❌ Survived  
   - **Reason**: Original test lacked both bounds as `NaN`.  
   - **Fix**: Added `testIsNaNRangeTrue()` to target this specific logic.

10. **Mutant in `expandToInclude()` (did not expand when value == lower)**  
    - **Status**: ❌ Survived  
    - **Reason**: Edge case untested.  
    - **Fix**: Added `testExpandToInclude_ValueBelowRange()` and `testExpandToInclude_ValueInsideRange()`.

This analysis shows that many of the original tests were sufficient to catch common mutations, especially around math and bounds logic. However, several logic-based and boundary-specific mutants survived, requiring a focused testing strategy to detect and eliminate them.

### 2.3 Report all the statistics and the mutation score for each of the mutated classes (Range and DataUtilities) with each test suite class (original and the updated one)

Mutation testing was applied to two main classes: `Range.java` and `DataUtilities.java`. Both classes were initially tested with the existing JUnit test suites provided in the assignment (referred to here as the "original" test suites). Then, each test suite was significantly expanded with custom test cases designed specifically to improve mutation coverage (referred to as the "updated" test suites).

Below are the mutation testing results for each class using PIT:

#### Range.java
- **Original Mutation Score**: 80%  
  - **Mutants Generated**: 1239  
  - **Mutants Killed**: 991  
  - **Survived**: 248  
  - ![Original Mutation Coverage for Range](<Include screenshot here>)
  - ![](./media/2-Preferances.png)

- **Updated Mutation Score**: 81%  
  - **Mutants Killed**: 998  
  - **Survived**: 241  
  - This improvement was achieved through the addition of over 50 custom test cases that addressed subtle edge conditions and previously uncovered branches.
  - ![Updated Mutation Coverage for Range](<Include screenshot here>)

#### DataUtilities.java
- **Original Mutation Score**: ~55% (based on team log and screenshots)  
  - **Mutants Generated**: 775  
  - **Mutants Killed**: ~426  
  - ![Original Mutation Coverage for DataUtilities](<Include screenshot here>)

- **Updated Mutation Score**: 82%  
  - **Mutants Killed**: 634  
  - This increase was accomplished by adding extensive test cases covering null handling, 2D arrays with irregular dimensions, edge rounding behavior, and exception validation.
  - ![Updated Mutation Coverage for DataUtilities](<Include screenshot here>)

These improvements required focused effort in understanding the mutation logs, reverse engineering untested branches, and repeatedly tuning test assertions to verify that mutants were being effectively killed.

### 2.4 A discussion on the effect of equivalent mutants on mutation score accuracy including a discussion on how equivalent mutants could be detected

Equivalent mutants are syntactic variations of the original code that, despite appearing different at the bytecode level, behave identically for all inputs. This makes them indistinguishable from the original program through black-box testing and, as a result, impossible to kill using any test case. Their presence negatively affects the perceived effectiveness of a test suite because they artificially lower the mutation score, giving the impression that more mutants have survived due to inadequate testing.

In this assignment, we encountered multiple instances of equivalent mutants, particularly in the `Range` and `DataUtilities` classes. For example, mutants that replaced conditional checks like `value < lower` with `value <= lower` or arithmetic operations that had no impact due to internal rounding behavior or symmetric inputs (like scaling a range with a factor of 1.0) were practically equivalent. Despite our best efforts to design tests around them, these mutants consistently survived, confirming their equivalence.

To illustrate this, PIT generated mutants in methods like `Range.getCentralValue()` where small changes in arithmetic (e.g., replacing `+` with `-`) survived even though the result was the same due to symmetric inputs (like [-1.0, 1.0] resulting in 0.0 either way). Similarly, some mutants in `DataUtilities.createNumberArray2D()` survived because they altered code that either handled null/empty input gracefully or were effectively unreachable during realistic execution.

Detecting equivalent mutants automatically remains a challenging problem in mutation testing. However, some manual and semi-automated strategies were used in this lab:

- **Manual Review of Survived Mutants**: We inspected the mutation reports (e.g., in `Range.java.html` and `index.html`) to identify lines where mutants survived but were logically equivalent.
- **Behavioral Inference**: For surviving mutants that involved no observable output difference even after edge-case testing, we flagged them as equivalent.
- **Isolated Debugging**: In cases of persistent survivors, we manually traced values and verified identical control flow and outcomes under all practical test data.

While we could not remove these equivalent mutants from the report, we acknowledged them in our analysis and marked them to avoid wasting time trying to design impossible-to-pass tests. Recognizing their presence helped us accurately interpret the mutation score and test suite effectiveness.

In conclusion, equivalent mutants skew mutation scores and require careful interpretation. They highlight the limitation of mutation testing tools and emphasize the importance of combining mutation testing with developer insight and source-level understanding.

### 2.5 A discussion of how you improved the mutation score of the test suites. Your design strategy.

The mutation score improvements for both `Range` and `DataUtilities` classes were achieved through a structured, iterative design strategy focused on eliminating surviving mutants, addressing weak spots in the original test suite, and enhancing overall test coverage. The process was guided by detailed analysis of PIT-generated mutation reports, with each cycle of improvement driven by targeted goals.

#### Step 1: Analyzing Surviving Mutants
We began by running mutation testing with PIT on the original test suites. The mutation report highlighted which lines of code were being mutated, and which mutants had survived. We then carefully examined those surviving mutants to determine whether they represented meaningful logic changes or were equivalent mutants. This gave us a roadmap of which behaviors and edge cases our original tests failed to cover.

#### Step 2: Prioritizing High-Risk Methods
We focused initially on public-facing methods with complex conditional logic or multiple execution paths, such as `Range.shift()`, `Range.intersects()`, `Range.getCentralValue()`, `DataUtilities.createNumberArray2D()`, and `DataUtilities.getCumulativePercentages()`. These methods are central to the core functionality of their respective classes and more likely to contain logic that affects correctness.

#### Step 3: Designing Edge Case Tests
For each method, we introduced multiple test cases covering:
- Edge boundary values (e.g., exactly at lower and upper bounds)
- Invalid inputs (null arrays, empty arrays, reverse ranges)
- Floating-point precision differences
- Zero-length ranges, NaN values, and symmetric ranges
- Conditional logic boundaries and branch inversions

This phase alone helped kill dozens of mutants that had survived the original test suite.

#### Step 4: Iterative Testing and Refinement
We re-ran mutation testing after each batch of new test cases. If the score didn’t improve, we returned to the report and re-analyzed the surviving mutants. In many cases, surviving mutants pointed to test cases that were technically correct but lacked precise assertions or did not trigger the relevant logic path. We refined such tests or added variants with alternate input values to force the mutated branch to be exercised.

#### Step 5: Identifying and Isolating Equivalent Mutants
When tests could not kill a mutant despite high coverage and precision, we began tracking them as likely equivalent mutants. These were documented separately and excluded from further test writing to avoid wasting time.

#### Step 6: Targeting Remaining Functional Gaps
By this phase, we targeted utility methods and defensive code (e.g., clamping logic in `shift()`, cumulative calculations in `DataUtilities`). We added tests for corner-case behaviors involving minimal inputs, invalid arguments, or fallbacks.

#### Final Outcome
The `Range` class improved from 80% to 81% mutation coverage, and `DataUtilities` increased dramatically from approximately 55% to 82%. Our strategy demonstrated that with focused, mutation-driven design, test quality can be objectively improved by discovering gaps that traditional code coverage tools might miss.

In summary, our strategy emphasized:
- Feedback loops from PIT reports
- Design of test inputs targeting specific paths and mutants
- Iterative refinement and debugging
- Awareness of test limitations and equivalent code

This approach not only improved mutation scores but made our test suites significantly more robust and meaningful.

### 2.6 A discussion of what could have been done to improve the mutation score of the test suites

Despite significant gains in mutation coverage, especially for `DataUtilities`, there remain additional strategies that could have been employed to further improve the mutation score:

1. **Use of Parameterized Testing**: Many of the test cases had similar structure with varying inputs. Refactoring them into parameterized tests using JUnit's `@ParameterizedTest` could have allowed more systematic exploration of value permutations, particularly boundary-adjacent cases.

2. **White-box Analysis Tools**: Integrating static analysis tools or control flow visualizers could have helped identify under-tested branches or decision points missed by the test suite.

3. **Reflection-Based Execution**: Some code paths (e.g., private method behavior) were not directly testable via public methods. Employing reflection to explicitly invoke private methods or internal branches could have helped access otherwise unreachable code.

4. **Mocking External Conditions**: Though not heavily required in our assignment, use of mocking (e.g., via Mockito) could be useful in future assignments to simulate exceptional states or failover conditions that might reveal hidden mutants.

5. **Selective Mutator Configuration**: While we used the default set of mutators from PIT, customizing the mutator configuration (e.g., enabling condition boundary or switch mutators) may have highlighted alternate or subtler faults.

6. **Review of External Test Data**: Incorporating randomized input generators or fuzz testing libraries might help stress-test input edge conditions and kill logic mutants not caught by manually designed tests.

7. **Time-Boxed Equivalent Mutant Classification**: More structured classification of equivalent mutants using domain logic analysis or compiler-optimized output comparison could have helped identify and justify equivalency rather than relying on trial and error.

### 2.6.1 Analysis drawn on the effectiveness of each of the test classes

#### RangeTest.java
- **Original Score**: 80%  
- **Final Score**: 81%  
- **Observations**: The original test suite provided good branch and boundary coverage. However, it lacked depth in exploring cross-zero behaviors, small deltas, NaN conditions, and symmetric edge values. The updated test suite added over 40 new test cases specifically targeting shift logic, clamping behavior, intersection conditions, and floating-point rounding, which collectively contributed to a modest 1% gain.

#### DataUtilitiesTest.java
- **Original Score**: ~55%  
- **Final Score**: 82%  
- **Observations**: The original test suite missed significant logic in handling nulls, empty arrays, malformed input, and floating-point accumulation behavior. The improved suite introduced detailed tests for 2D array conversions, cumulative percentages, and exception paths, which dramatically increased killed mutants. This class benefited the most from targeted testing and mutant analysis.

#### TestRunner.java
- **Mutation Score**: 0%  
- **Explanation**: This class only served as a harness or launcher and did not contain logical code relevant to mutation. It was excluded from test target classes after the initial run to prevent score pollution.

Overall, the custom test classes significantly improved the fault-detection ability of the test suites. `DataUtilitiesTest` showed the most effective improvement, while `RangeTest` achieved high coverage but faced diminishing returns due to a concentration of equivalent or unreachable mutants.


### 2.7 Why do we need mutation testing? A discussion on the advantages and disadvantages of mutation testing

Mutation testing plays a crucial role in assessing the quality and robustness of a software test suite. Unlike conventional code coverage metrics (such as line or branch coverage), mutation testing evaluates how well a test suite can detect injected faults that simulate common developer mistakes. This provides a more realistic measure of fault detection capability and promotes the development of stronger, more reliable tests.

#### Advantages of Mutation Testing

1. **Reveals Test Weaknesses**: Mutation testing can uncover gaps in a test suite that traditional coverage metrics fail to detect. Even 100% line or branch coverage does not guarantee that logic has been thoroughly tested; surviving mutants often highlight this.

2. **Encourages High-Quality Test Design**: By attempting to kill mutants, developers are pushed to consider edge cases, invalid inputs, and deeper logical paths they may otherwise overlook.

3. **Objective Metric for Test Effectiveness**: Mutation score offers a clear, quantifiable measure of how fault-tolerant the test suite is, allowing comparisons across codebases or iterations.

4. **Highlights Equivalent or Dead Code**: When a mutant cannot be killed regardless of the input, it may reveal equivalent code or unreachable logic, prompting code refactoring or clarification.

5. **Complements Static Analysis Tools**: While static analysis identifies potential problems, mutation testing confirms whether the test suite can detect them when they occur at runtime.

#### Disadvantages of Mutation Testing

1. **Performance Overhead**: Generating and testing thousands of mutants is computationally expensive and time-consuming, particularly for large codebases.

2. **Equivalent Mutants Problem**: Mutation testing tools cannot always distinguish between logical and equivalent mutants. Developers must manually inspect surviving mutants, which is labor-intensive and sometimes inconclusive.

3. **Complex Tooling and Integration**: Tools like PIT may require configuration tuning, build system adjustments, or workaround for classpath and test scope issues, increasing setup time.

4. **False Sense of Security**: High mutation score does not always mean the system is fault-free. It only indicates that the tests can detect a predefined set of code-level changes, not all types of software errors.

5. **Limited Scope**: Mutation testing is mainly effective for unit tests. Its usefulness diminishes when applied to integration or system-level testing.

#### Summary

Despite its limitations, mutation testing provides deep insights into test quality and strengthens confidence in the software’s reliability. When used alongside traditional metrics and modern CI/CD pipelines, it significantly enhances the rigor of software verification processes.


### 2.8 A discussion on how the team work/effort was divided and managed. Any lessons learned from your teamwork on this lab?

Our team consisted of four members, and we adopted a collaborative and iterative workflow throughout the lab to maximize productivity and ensure quality contributions from everyone. The tasks were divided based on logical components and stages of the assignment.

We began by collectively exploring the assignment requirements and discussing the structure of the report. During this phase, each member took responsibility for reviewing specific parts of the documentation and relevant PIT documentation to ensure we all had a clear understanding of mutation testing principles.

#### Collaborative Sections:
- **Section 2.5.5: Analysis of Mutants for Range Class** – This was completed as a group. We met multiple times to review the mutants generated by PIT for the `Range` class, discussed the ones that survived, and brainstormed possible reasons and test strategies. Each member contributed example test cases, and we selected the most effective ones collaboratively.
- **Section 2.5.6: Equivalent Mutants Detection** – Also completed together. We reviewed the mutation reports as a team, using screen sharing to analyze the `Range.java.html` and `DataUtilities.java.html` reports. We debated and agreed on which mutants were truly equivalent, and how best to document them.

#### Pair Programming Sessions:
- **Section 2.5.7: Improving Mutation Scores** – For this section, the team split into two pairs. One pair focused on improving the mutation score for `DataUtilities.java`, while the other pair worked on `Range.java`. Each pair was responsible for analyzing the PIT report, adding new test cases, verifying improvements, and resolving any test failures. This parallel effort significantly accelerated the progress and allowed each pair to specialize in the structure and logic of their assigned class.

#### Team Coordination:
- We used a shared document and a version control system to track and merge contributions. Regular check-ins helped us synchronize progress, raise questions, and ensure consistency in formatting and tone.
- Tasks were sometimes revisited in rotation, where one member reviewed and validated the tests or documentation written by others.

#### Lessons Learned:
- **Value of Collaboration**: Discussing complex mutant behavior as a group improved our understanding and helped catch logic errors we might have missed individually.
- **Importance of Specialization**: Dividing test suite improvements by class allowed deeper familiarity and more targeted testing strategies.
- **Communication is Key**: Clear communication helped us avoid duplication of work and resolve blocking issues more efficiently.
- **Time Management**: Pairing and group sessions were more effective when scheduled in advance with clear objectives.

Overall, the teamwork experience on this lab was positive, productive, and well-balanced. The collaborative model helped us deliver high-quality results while also enhancing each member’s individual understanding of mutation testing practices.


### 2.9 Difficulties encountered, challenges overcome, and lessons learned

Throughout the course of this assignment, our team encountered several technical and conceptual challenges. These hurdles spanned across tool configuration, platform compatibility, test design complexity, and mutant classification. However, each challenge led to a learning opportunity and ultimately enhanced our understanding of mutation testing.

#### JDK Compatibility Issues
One of the earliest and most critical challenges was an incompatibility between PIT and the installed Java version. Our initial setup used JDK 21, which led to the following runtime error:

```
Unsupported class file major version 67
```

After extensive troubleshooting and research, we discovered that PIT at the time only supported Java up to version 17. To resolve this, we:
- Installed JDK 17
- Added it to Eclipse’s Installed JREs
- Set it as the default runtime for the workspace and project
- Recompiled all test classes

This resolved the compatibility issue and allowed mutation testing to proceed successfully. This experience reinforced the importance of environment compatibility when working with advanced testing tools.

#### Challenges in Section 2.5.5 – Mutant Analysis for Range
Analyzing mutants for `Range` using PIT’s HTML reports required careful manual inspection. Some mutants involved changes in logical operators or boundary conditions, making it difficult to determine whether they were truly equivalent or just poorly tested. Group discussions helped validate these edge cases and led to a focused effort on boundary-based test additions.

#### Challenges in Section 2.5.6 – Equivalent Mutants
Identifying equivalent mutants proved to be particularly challenging. Despite writing seemingly thorough tests, some mutants remained undetected. We had to:
- Trace through source code manually
- Confirm the functional equivalence of mutated and original logic
- Rule out input sets that could differentiate them

This process was time-intensive, but it taught us the importance of not blindly chasing every surviving mutant and instead justifying their status as equivalent when appropriate.

#### Challenges in Section 2.5.7 – Improving Mutation Scores
Improving mutation scores, especially beyond 80%, required highly targeted test cases. For the `Range` class, some logic was deeply nested or conditional on rare input combinations. For `DataUtilities`, special handling was required for malformed 2D arrays, null inputs, and cumulative percentage calculations.

Additional issues included:
- Mutation survivors caused by off-by-one logic or floating-point rounding
- Overlapping assertions or duplicate tests that did not increase coverage
- Breakpoints and assertions not being triggered despite line coverage being 100%

We addressed these by:
- Refactoring test methods for precision
- Using debugging and logging to confirm code path execution
- Verifying logic against PIT's HTML diff output

#### Lessons Learned
- **Tool Awareness**: Even mature tools like PIT have version constraints and behavioral quirks.
- **Strategic Testing**: Not all mutants need to be killed. Distinguishing between equivalent and inadequately-tested mutants is critical.
- **Importance of Environment**: Toolchain consistency (JDK, IDE, build paths) can significantly affect test outcomes.
- **Team-Based Debugging**: Complex mutant behavior is easier to analyze and fix with collaborative reasoning.

These challenges made the process intensive but ultimately more rewarding. We walked away with a deeper appreciation for mutation testing and how it reveals both the strengths and blind spots of a test suite.

### 2.10 Comments/feedback on the assignment itself

Overall, this assignment was challenging yet deeply educational. It provided us with practical experience in a sophisticated area of software testing that is rarely covered in traditional unit testing exercises. Mutation testing forced us to think beyond basic coverage metrics and analyze our test cases with greater precision and intent.

The structure of the assignment, which included progressive tasks—starting from mutant analysis, to test suite enhancement, to evaluation of equivalent mutants—was effective in gradually building our understanding. The requirement to incrementally improve our mutation score gave us a clear goal and motivation throughout the lab.

The integration with PIT was beneficial, although some configuration issues (especially related to Java versioning) added overhead early in the process. However, resolving these issues provided valuable insight into the real-world challenges of tool integration and compatibility.

The mutation reports generated by PIT were very useful for identifying weaknesses in our test suite. However, more guidance or examples on interpreting these reports would have made the early phases easier to approach, especially for first-time users.

We especially appreciated that the assignment encouraged collaboration and discussion. Tasks like identifying equivalent mutants and reasoning about logical changes were ideal for group work and taught us how to better articulate and debate software behavior.

In conclusion, this was one of the most valuable testing labs we've done. It not only deepened our understanding of test quality and coverage but also taught us how to use tools like PIT to strengthen our test suites with a more systematic approach. We would recommend minor tooling support additions and a few more introductory examples to further improve the experience for future students.
